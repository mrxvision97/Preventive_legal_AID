# Quick Start: Test Offline Mode on Laptop

## âœ… Prerequisites Check

1. **Ollama installed and running:**
```bash
ollama list
# Should show your models
```

2. **Model downloaded:**
```bash
ollama list
# Should show: llama3.2:3b
```

## ğŸš€ Quick Setup (3 Steps)

### Step 1: Configure `.env`

Create `backend/.env` file:

```bash
# Offline Mode Configuration
USE_OFFLINE_MODE=true
OLLAMA_MODEL=llama3.2:3b
OLLAMA_BASE_URL=http://localhost:11434

# Leave OpenAI empty for offline
OPENAI_API_KEY=
```

### Step 2: Start Ollama (if not running)

```bash
# In a separate terminal
ollama serve
```

### Step 3: Start Backend

```bash
cd backend
uvicorn app.main:app --reload
```

## ğŸ§ª Quick Test

Run the test script:

```bash
python backend/scripts/test_offline_quick.py
```

This will:
- âœ… Check Ollama connection
- âœ… Verify model availability
- âœ… Test a sample query
- âœ… Show results

## ğŸ“ Manual API Test

```bash
curl -X POST "http://localhost:8000/api/v1/public/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are my rights if my landlord increases rent?",
    "domain": "civil",
    "language": "en"
  }'
```

## âš¡ Expected Results

- **Response Time**: 5-15 seconds (first query may be slower)
- **Model Used**: llama3.2:3b
- **Quality**: Good (llama3.2:3b is a quality model)

## ğŸ”§ Troubleshooting

### Ollama not running?
```bash
ollama serve
```

### Model not found?
```bash
ollama pull llama3.2:3b
```

### Still using OpenAI?
- Check `.env`: `USE_OFFLINE_MODE=true`
- Restart backend server

## âœ… Success Indicators

- âœ… Backend starts without errors
- âœ… Test script completes successfully
- âœ… API returns legal analysis
- âœ… Response time is reasonable (5-15 seconds)

If all check, you're ready! ğŸ‰

